{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "440deaf1-cc7f-4213-9107-dda22663b0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "dir2 = os.path.abspath('')\n",
    "dir1 = os.path.dirname(dir2)\n",
    "if not dir1 in sys.path: sys.path.append(dir1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9764780-bd92-44c9-af87-499397d95c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brio.utils.Preprocessing import Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pickle import dump, load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from functools import reduce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc922c1a-d0f3-47e1-ab5e-68be2a096fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brio.bias.FreqVsFreqBiasDetector import FreqVsFreqBiasDetector, BiasDetector\n",
    "from brio.risk.HazardFromBiasDetectionCalculator import HazardFromBiasDetectionCalculator\n",
    "from brio.risk.RiskCalculator import RiskCalculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa234d3-f7a7-40b7-b172-4fedc02d3883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.datasets import StandardDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfa4f7e-68b5-451d-9c6d-0d619b43573c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_path = \"../data/CRIF_POC/\"\n",
    "output_data_path = \"../data/output/\"\n",
    "total = pd.read_csv(input_data_path + 'german_credit_crif.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bf8f08-c928-461f-828c-7e1afb6a5fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_german_credit(df_german):\n",
    "    df_num = df_german.copy()\n",
    "    gender_dic={'A91': 1, 'A92': 2, 'A93': 1, 'A94': 1, 'A95': 2} #1=male, 2=female\n",
    "    personal_status_dic={'A91': 2, 'A92': 2, 'A93': 1, 'A94': 2, 'A95': 1} #1=single, 2=married/separated/widowed/divorced\n",
    "\n",
    "    df_num['age_group']=pd.cut(df_german[\"age\"], #todo change in case\n",
    "       bins=[0, 27, 37, 47, 100],\n",
    "       labels=[1, 2, 3, 4]).astype('int64')\n",
    "    df_num['gender']=df_german['personal_status_and_sex'].map(gender_dic)\n",
    "    df_num['marital_status']=df_german['personal_status_and_sex'].map(personal_status_dic)\n",
    "\n",
    "    attribute1_dic={'A11': 1, 'A12': 2, 'A13': 3, 'A14': 4}\n",
    "    attribute3_dic={'A30': 0, 'A31': 1, 'A32': 2, 'A33': 3, 'A34': 4}\n",
    "    attribute4_dic={'A40': 0, 'A41': 1, 'A42': 2, 'A43': 3, 'A44': 4, 'A45': 5, 'A46': 6, 'A47': 7, 'A48': 8, 'A49': 9, 'A410': 10}\n",
    "    attribute6_dic={'A61': 1, 'A62': 2, 'A63': 3, 'A64': 4, 'A65': 5}\n",
    "    attribute7_dic={'A71': 1, 'A72': 2, 'A73': 3, 'A74': 4, 'A75': 5}\n",
    "    attribute9_dic={'A91': 1, 'A92': 2, 'A93': 3, 'A94': 4, 'A95': 5}\n",
    "    attribute10_dic={'A101': 1, 'A102': 2, 'A103': 3}\n",
    "    attribute12_dic={'A121': 1, 'A122': 2, 'A123': 3, 'A124': 4}\n",
    "    attribute14_dic={'A141': 1, 'A142': 2, 'A143': 3}\n",
    "    attribute15_dic={'A151': 1, 'A152': 2, 'A153': 3}\n",
    "    attribute17_dic={'A171': 1, 'A172': 2, 'A173': 3, 'A174': 4}\n",
    "    attribute19_dic={'A191': 1, 'A192': 2}\n",
    "    attribute20_dic={'A201': 1, 'A202': 2}\n",
    "\n",
    "\n",
    "    \n",
    "    df_num['status']=df_german['status'].map(attribute1_dic)\n",
    "    df_num['duration']=df_german['duration']\n",
    "    df_num['credit_history']=df_german['credit_history'].map(attribute3_dic)\n",
    "    df_num['purpose']=df_german['purpose'].map(attribute4_dic)\n",
    "    df_num['credit_amount']=df_german['credit_amount']\n",
    "    df_num['savings/bonds']=df_german['savings/bonds'].map(attribute6_dic)\n",
    "    df_num['employment_duration']=df_german['employment_duration'].map(attribute7_dic)\n",
    "    df_num['installment_rate']=df_german['installment_rate']\n",
    "    df_num['personal_status_and_sex']=df_german['personal_status_and_sex'].map(attribute9_dic)\n",
    "    df_num['other_debtors']=df_german['other_debtors'].map(attribute10_dic)\n",
    "    df_num['residence_duration']=df_german['residence_duration']\n",
    "    df_num['property']=df_german['property'].map(attribute12_dic)\n",
    "    df_num['age']=df_german['age']\n",
    "    df_num['installment_plans']=df_german['installment_plans'].map(attribute14_dic)\n",
    "    df_num['housing']=df_german['housing'].map(attribute15_dic)\n",
    "    df_num['number_of_credits']=df_german['number_of_credits']\n",
    "    df_num['job']=df_german['job'].map(attribute17_dic)\n",
    "    df_num['people_liable']=df_german['people_liable']\n",
    "    df_num['telephone']=df_german['telephone'].map(attribute19_dic)\n",
    "    df_num['foreign_worker']=df_german['foreign_worker'].map(attribute20_dic)\n",
    "    \n",
    "    df_num['credit_risk']=df_german['credit_risk']\n",
    "    df_num['predicted_label']=df_german['predicted_label'].apply(np.int64)\n",
    "    df_num.head(10)\n",
    "    return df_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbf54b8-648f-4d70-8afc-d2662d34a98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fair_metrics(dataset, ground_truth, sensitive):\n",
    "    \n",
    "    privileged_groups = [{sensitive: 1}]    \n",
    "    unprivileged_groups = [{sensitive: 0}]\n",
    "\n",
    "    \n",
    "    binary = BinaryLabelDatasetMetric(dataset, \n",
    "                                       unprivileged_groups=unprivileged_groups, \n",
    "                                       privileged_groups=privileged_groups)\n",
    "    disp_impact =  binary.disparate_impact().round(2) # Pr(Y = 1 | D = \\text{unprivileged})/Pr(Y = 1 | D = \\text{privileged})\n",
    "    print(disp_impact)\n",
    "\n",
    "\n",
    "    \n",
    "    dataset_pred = ground_truth\n",
    "    dataset_pred.labels = dataset.labels\n",
    "    classif = ClassificationMetric(\n",
    "                                  ground_truth,\n",
    "                                  dataset_pred,\n",
    "                                  unprivileged_groups=unprivileged_groups, \n",
    "                                  privileged_groups=privileged_groups    \n",
    "                                  )\n",
    "\n",
    "    fnrr = classif.false_negative_rate_difference()\n",
    "    print(fnrr)\n",
    "\n",
    "\n",
    "    return disp_impact, fnrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec18ece8-5b09-4d12-b15f-c5bfa4d24319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_freq_v_freq(df_num, mode, sensitive_list, threshold, business_list):\n",
    "    #display(df_num.head())\n",
    "    results = {}\n",
    "    results_aif = {}\n",
    "    fnrr_results_aif = {}\n",
    "    results_aggr = []\n",
    "    expl_list = []\n",
    "    id_list = []\n",
    "    dict_sens = {'foreign_worker': [2], \n",
    "                 'personal_status_and_sex': [4], \n",
    "                 'age_group': [4], \n",
    "                 'gender': [1],              \n",
    "                 'marital_status': [0]\n",
    "                } \n",
    "    skip_these_features = ['id', 'predicted_label', 'credit_risk', 'age']\n",
    "    \n",
    "    if t == None:\n",
    "         bd = FreqVsFreqBiasDetector(distance=\"TVD\", A1='high')\n",
    "    else: \n",
    "        bd = FreqVsFreqBiasDetector(distance=\"TVD\")\n",
    "    hc = HazardFromBiasDetectionCalculator()\n",
    "    \n",
    "    #business_list = ['status', 'duration', 'credit_history'] #[c for c in df_num.columns if (c not in skip_these_features and c not in sensitive_list)]\n",
    "    #print(business_list)\n",
    "\n",
    "    # explainability\n",
    "    expl = {}\n",
    "    for variable in df_num.columns:\n",
    "       # print(variable)\n",
    "        if variable in skip_these_features:\n",
    "            continue\n",
    "\n",
    "        demo_parity = bd.compare_root_variable_groups(\n",
    "            dataframe=df_num,\n",
    "            target_variable='predicted_label',\n",
    "            root_variable=variable,\n",
    "            threshold=threshold\n",
    "        )   \n",
    "        expl[variable] = demo_parity['distance']\n",
    "\n",
    "    \n",
    "    expl = pd.DataFrame(expl.items(), columns=['Feature', 'Distance']).sort_values(by='Distance', ascending=False)\n",
    "\n",
    "    if business_list == None:\n",
    "            business_list = expl['Feature'].head(3).values\n",
    "    expl_df = expl[expl['Feature'].isin(business_list)].reset_index(drop=True)\n",
    "    print('Conditioning features: ' + str(business_list))\n",
    "    \n",
    "    for sensitive in sensitive_list:\n",
    "        # demographic parity\n",
    "        demo_parity = bd.compare_root_variable_groups(\n",
    "            dataframe=df_num,\n",
    "            target_variable='predicted_label',\n",
    "            root_variable=sensitive,\n",
    "            threshold=threshold\n",
    "        )   \n",
    "        \n",
    "        #conditional demographic parity\n",
    "        print('---------' + sensitive)\n",
    "        cond_demo_parity = bd.compare_root_variable_conditioned_groups(\n",
    "            dataframe=df_num,\n",
    "            target_variable='predicted_label',\n",
    "            root_variable=sensitive,\n",
    "            conditioning_variables=business_list,\n",
    "            threshold=threshold,\n",
    "            min_obs_per_group=30\n",
    "            )        \n",
    "        print(demo_parity)\n",
    "        #print(cond_demo_parity)\n",
    "        \n",
    "        # # intersectional fairness\n",
    "        # intersectional = bd.compare_root_variable_conditioned_groups(\n",
    "        # dataframe=df_num,\n",
    "        # target_variable='predicted_label',\n",
    "        # root_variable=sensitive,\n",
    "        # conditioning_variables=[s for s in sensitive_list if s != sensitive],\n",
    "        # )\n",
    "        # print(intersectional)\n",
    "        # results.append(intersectional)\n",
    "\n",
    "        # hazard cioè rischio del singolo test\n",
    "        hazard = hc.compute_hazard_from_freqvsfreq_or_freqvsref(\n",
    "            demo_parity,\n",
    "            cond_demo_parity,\n",
    "            df_num.shape[0],\n",
    "            business_list,\n",
    "            weight_logic=mode\n",
    "        )\n",
    "        results[sensitive] = hazard\n",
    "        results_aggr.append(hazard)\n",
    "                  \n",
    "        \n",
    "        dataset_orig = StandardDataset(df_num,\n",
    "                                       label_name='predicted_label',\n",
    "                                       protected_attribute_names=[sensitive],\n",
    "                                       privileged_classes=[lambda x: 1 if x == dict_sens[sensitive][0] else 0], \n",
    "                                       favorable_classes=[0]\n",
    "                                        )       \n",
    "\n",
    "        ground_truth = StandardDataset(df_num,\n",
    "                               label_name='credit_risk',\n",
    "                               protected_attribute_names=[sensitive],\n",
    "                               privileged_classes=[lambda x: 1 if x == dict_sens[sensitive][0] else 0], \n",
    "                               favorable_classes=[0]\n",
    "                                )              \n",
    "        \n",
    "        print(fair_metrics(dataset_orig, ground_truth, sensitive))\n",
    "        a, b = fair_metrics(dataset_orig, ground_truth, sensitive)\n",
    "        results_aif[sensitive] = a\n",
    "        fnrr_results_aif[sensitive] = b\n",
    "        \n",
    "        if fair_metrics(dataset_orig, ground_truth, sensitive)[0]<1: #this means that the unprivileged class is disadvantaged\n",
    "            display(df_num[~df_num[sensitive].isin(dict_sens[sensitive])].head())            \n",
    "            exposure = df_num[~df_num[sensitive].isin(dict_sens[sensitive])]['id'].values\n",
    "            id_list.append(exposure)\n",
    "            print(exposure)\n",
    "            \n",
    "        elif fair_metrics(dataset_orig, ground_truth, sensitive)[0]>1:\n",
    "            display(df_num[df_num[sensitive].isin(dict_sens[sensitive])].head())\n",
    "            exposure = df_num[df_num[sensitive].isin(dict_sens[sensitive])]['id'].values\n",
    "            id_list.append(exposure)\n",
    "            print(exposure)\n",
    "            \n",
    "    print('intersection on ' +  str(len(id_list)) + ' sets:')\n",
    "    print(len(list(reduce(lambda x, y: set(x).intersection(y), id_list))))\n",
    "    \n",
    "    print('vuln_non_pesata' + str(len(list(reduce(lambda x, y: set(x).intersection(y), id_list)))/df_num.shape[0]))\n",
    "    vuln = len(list(reduce(lambda x, y: set(x).intersection(y), id_list)))/df_num.shape[0]*len(id_list)\n",
    "    \n",
    "    #print('Results for ' + mode + ' mode')\n",
    "    # print(results)\n",
    "    print(results_aggr)\n",
    "    print('vuln ' + str(vuln))\n",
    "\n",
    "    return results_aggr, results, results_aif, fnrr_results_aif, business_list, sensitive_list, vuln, expl_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e18027-3124-4a40-aebf-6ee3166247de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_german = process_german_credit(total)\n",
    "# total_german.to_csv(output_data_path + 'processed_german_credit_crif.csv', index=False, sep=',')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631aa9ab-2f47-4528-a5fb-ab8831d001bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_expl(list, df_num, i, label):\n",
    "\n",
    "    for feat in list:\n",
    "        \n",
    "        # Create a figure and two subplots on the same row\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(8,4))  # 1 row, 2 columns, adjust figsize as needed\n",
    "        \n",
    "        # First plot (histogram)\n",
    "        sns.histplot(\n",
    "            data=df_num, \n",
    "            x=feat, \n",
    "            hue='predicted_label', \n",
    "            multiple=\"dodge\", \n",
    "            palette={1: 'red', 0: 'blue'},\n",
    "            alpha=0.5,\n",
    "            ax=axes[0]  # Assign to the first subplot\n",
    "        )\n",
    "        axes[0].set_title('Histogram')\n",
    "        \n",
    "        # Second plot (bar plot)\n",
    "        df_num.groupby([feat])['predicted_label'].mean().plot.bar(\n",
    "            ax=axes[1],\n",
    "            color='green'\n",
    "            # Assign to the second subplot\n",
    "    \n",
    "        )\n",
    "        axes[1].set_title('Mean risk score')\n",
    "    \n",
    "        fig.savefig(output_data_path + '/images/' + f'{label}_{feat}_model0' + str(i+1))\n",
    "        # Adjust layout to prevent overlapping\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efc8fe3-519a-4d48-90a1-cca7c19fe551",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prime 3 features per correlation (se la lista è None, verranno scelte le 3 feature con gli hazard più alti)\n",
    "business_list_by_model = {0:['status', 'duration', 'credit_history'],\n",
    "                          1:['status', 'duration', 'credit_history'],\n",
    "                          2:['status', 'property', 'credit_history'],\n",
    "                          3:['status', 'duration', 'credit_history'],\n",
    "                          4:['credit_history', 'number_of_credits', 'employment_duration'],\n",
    "                          5:None\n",
    "                         }\n",
    "\n",
    "# business_list_by_model = {0:None,\n",
    "#                           1:None,\n",
    "#                           2:None,\n",
    "#                           3:None,\n",
    "#                           4:None,\n",
    "#                           5:None\n",
    "#                          } \n",
    "\n",
    "for t in [0.15]:\n",
    "    data = []\n",
    "    list_group_fairness = []\n",
    "    list_ind_fairness = []\n",
    "    aif_table = []\n",
    "    condit_feat = []\n",
    "    sensitive_list = ['foreign_worker', 'personal_status_and_sex', 'age_group']\n",
    "    threshold=t\n",
    "    modes = ['group']#, 'individual']\n",
    "    \n",
    "    for i in range(6):\n",
    "    \n",
    "        print('************* MODEL 0' + str(i+1) )    \n",
    "        \n",
    "        dict = {}\n",
    "        lista = {}\n",
    "        condit_dict = {}\n",
    "        rc = RiskCalculator()\n",
    "        if i == 5:\n",
    "            model = pd.read_csv(output_data_path + 'processed_model0' + str(i+1) + '.csv').sort_values(by='id')\n",
    "            df_num = model.copy()\n",
    "    \n",
    "        else:\n",
    "            model = pd.read_csv(input_data_path + 'model0' + str(i+1) + '.csv').sort_values(by='id')\n",
    "            # print(model.shape)\n",
    "            print(model['predicted_label'].value_counts(normalize=True))\n",
    "            model = pd.merge(left=total, right=model, how='left', on='id')\n",
    "            model = model.dropna(subset=['predicted_label'])\n",
    "            df_num = process_german_credit(model)\n",
    "            df_num = df_num.rename(columns={\"savings/bonds\": \"savings_bonds\"})\n",
    "\n",
    "        #display(df_num.head())\n",
    "        #df_num.to_csv(output_data_path + 'processed_model0' + str(i+1) + '.csv', index=False, sep=',')  \n",
    "    \n",
    "    \n",
    "        # df_num.hist()\n",
    "        # pyplot.rcParams['figure.figsize']=[16,16]\n",
    "        # pyplot.show()\n",
    "    \n",
    "        # plt.figure(figsize=(10, 5))\n",
    "        # df_num.groupby(['personal_status_and_sex'])['predicted_label'].mean().plot.bar()\n",
    "        # plt.show()\n",
    "        # df_num.groupby(['gender'])['predicted_label'].mean().plot.bar()\n",
    "        # plt.show()\n",
    "        # df_num.groupby(['age_group'])['predicted_label'].mean().plot.bar()\n",
    "        # plt.show()\n",
    "        # df_num.groupby(['foreign_worker'])['predicted_label'].mean().plot.bar()\n",
    "        # plt.show()\n",
    "    \n",
    "        accuracy = (df_num['credit_risk'] == df_num['predicted_label']).mean()\n",
    "        dict['accuracy'] = accuracy\n",
    "        P = (df_num['predicted_label'] == 0).sum()  # Count where credit_risk is 1 (positive)\n",
    "        N = (df_num['predicted_label'] == 1).sum()  # Count where credit_risk is 0 (negative)\n",
    "        base_rate = P / (P + N)\n",
    "        dict['acceptance_rate'] = base_rate\n",
    "        print(business_list_by_model[i])\n",
    "        for mode in modes:\n",
    "            print('>>>>>>>>>>>>>>>>>>>>>>>>>' + mode + ' mode:')\n",
    "            hazards_aggr, hazards, aif, fnrr_aif, expl, sensitive_list, vuln, expl_df = calculate_freq_v_freq(df_num, mode, sensitive_list, threshold, business_list = business_list_by_model[i])\n",
    "            risk = rc.compute_risk([item[0] for item in hazards_aggr])\n",
    "            print(risk)        \n",
    "            dict[f'{mode}_risk'] = risk\n",
    "            dict[f'{mode}_vulnerability'] = vuln\n",
    "            if mode == 'group':\n",
    "                list_group_fairness.append(hazards)\n",
    "            else: \n",
    "                list_ind_fairness.append(hazards)\n",
    "                \n",
    "        condit_feat.append(expl_df)\n",
    "        plot_expl(expl, df_num, i, 'expl')\n",
    "        plot_expl(sensitive_list, df_num, i, 'sensitive')\n",
    "\n",
    "        # revenue analysis    \n",
    "        total_credit_amount=sum(df_num[(df_num['predicted_label']==0)]['credit_amount'])\n",
    "        # bad_rate=len(df_num[(df_num['performance']==1) & (df_num['score_bin']==0)])/len(df_num[(df_num['score_bin']==0)])\n",
    "        bad_rate=len(df_num[(df_num['predicted_label']==0) & (df_num['credit_risk']==1)])/len(df_num[(df_num['predicted_label']==0)])\n",
    "        provision=total_credit_amount*bad_rate*0.2\n",
    "        interest_rate = 1 #todo change this\n",
    "        profit = round(total_credit_amount*interest_rate-provision, 0)\n",
    "        dict['profit'] = profit\n",
    "        data.append(dict)\n",
    "        aif_table.append(aif)\n",
    "        #aif_table.append(fnrr_aif)\n",
    "    \n",
    "    print('individual mode')\n",
    "    ind_haz = pd.DataFrame(list_ind_fairness)\n",
    "    for col in ind_haz.columns:\n",
    "        ind_haz[f'{col}_relative_haz'] = ind_haz[col].apply(lambda x: x[0]/x[-1])\n",
    "        ind_haz[f'{col}_absolute_haz'] = ind_haz[col].apply(lambda x: x[0])\n",
    "    cols_rel = [col for col in ind_haz.columns if \"relative\" in col]\n",
    "    cols_abs = [col for col in ind_haz.columns if \"absolute\" in col]\n",
    "    ind_haz['overall_relative_haz'] =  ind_haz[cols_rel].mean(axis=1).apply(lambda x: '{:,.2%}'.format(x))\n",
    "    ind_haz['overall_absolute_haz'] =  ind_haz[cols_abs].mean(axis=1)\n",
    "\n",
    "    display(ind_haz)\n",
    "    \n",
    "    print('group mode')\n",
    "    group_haz = pd.DataFrame(list_group_fairness)\n",
    "    for col in group_haz.columns:\n",
    "        group_haz[f'{col}_relative_haz'] = group_haz[col].apply(lambda x: x[0]/x[-1])\n",
    "        group_haz[f'{col}_absolute_haz'] = group_haz[col].apply(lambda x: x[0])\n",
    "    cols_rel = [col for col in group_haz.columns if \"relative\" in col]\n",
    "    cols_abs = [col for col in group_haz.columns if \"absolute\" in col]\n",
    "    group_haz['overall_relative_haz'] =  group_haz[cols_rel].mean(axis=1).apply(lambda x: '{:,.2%}'.format(x))\n",
    "\n",
    "    group_haz['overall_absolute_haz'] =  group_haz[cols_abs].mean(axis=1)\n",
    "\n",
    "    display(group_haz)\n",
    "    \n",
    "    print('comparison')\n",
    "    results = pd.DataFrame(data)\n",
    "    display(results)\n",
    "    \n",
    "    print('AIF360 demo_parity (p(unpriv)/p(priv))')\n",
    "    aif_final = pd.DataFrame(aif_table)\n",
    "    display(aif_final)\n",
    "    \n",
    "    print('Most conditioning factors')\n",
    "    expl = pd.concat(condit_feat, axis=1)\n",
    "    display(expl)\n",
    "\n",
    "    with pd.ExcelWriter(output_data_path + f'threshold_{t}_results.xlsx') as writer:  \n",
    "        aif_final.to_excel(writer, sheet_name='AIF360_comparison')\n",
    "        results.to_excel(writer, sheet_name='results_overall')    \n",
    "        group_haz.to_excel(writer, sheet_name='group_mode_hazards')\n",
    "        ind_haz.to_excel(writer, sheet_name='individual_mode_hazards')\n",
    "        expl.to_excel(writer, sheet_name='explainability')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de53a8e-696a-4c17-a5cd-3c2d2d6786ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682fd896-d9b5-4266-8d7f-bdad7652e32c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
